# ğŸ“Š Data Pipeline Development â€“ Task 1 (CodTech Internship)

## ğŸ§‘â€ğŸ’» Project Description

This project implements a **complete ETL (Extract, Transform, Load) pipeline** using Python, `pandas`, and `scikit-learn`. It processes a raw dataset (`titanic.csv`), performs preprocessing and transformation, and outputs clean, ready-to-use machine learning data files.

---

## ğŸ“‚ Project Structure

.
â”œâ”€â”€ etl_pipeline.py # Main ETL script
â”œâ”€â”€ data/
â”‚ â””â”€â”€ titanic.csv # Raw input dataset
â”œâ”€â”€ processed_data/
â”‚ â”œâ”€â”€ features.csv # Preprocessed features (scaled + encoded)
â”‚ â””â”€â”€ target.csv # Target labels (e.g., Survived)
â””â”€â”€ README.md # Project documentation

---

## ğŸ› ï¸ Tools & Libraries Used

- Python 3.x
- pandas
- scikit-learn
- NumPy

---

## ğŸ” ETL Process Overview

1. **Extract**
   - Reads the raw Titanic dataset from `data/titanic.csv`.

2. **Transform**
   - Handles missing values.
   - Encodes categorical variables using `OneHotEncoder`.
   - Scales numerical features using `StandardScaler`.

3. **Load**
   - Saves processed features to `processed_data/features.csv`.
   - Saves target labels to `processed_data/target.csv`.

---

## â–¶ï¸ How to Run

1. **Install dependencies:**

```bash
pip install pandas scikit-learn numpy
